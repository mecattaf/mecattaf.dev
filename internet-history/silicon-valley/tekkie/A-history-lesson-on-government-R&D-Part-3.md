See [Part 1](https://tekkie.wordpress.com/2011/06/21/a-history-lesson-on-government-randd-part-1/), [Part 2](https://tekkie.wordpress.com/2012/02/04/a-history-lesson-on-government-rd-part-2/)

This post has been a long time coming. I really thought I was going to get it done about this time last year, but I got diverted into other research that I hope to convey on this blog in the future. I also found more sources to explore for the portion on Xerox PARC. Iâ€™ve been as faithful as I can to the history, but thereâ€™s always a possibility I made some mistakes. I welcome corrections from those who know better. ğŸ™‚

This is not a complete history of computing in the period I cover. I mean to convey the closing years of ARPAâ€™s â€œgreat leapsâ€ in ideas about computing, and then cover the attempt to continue those â€œgreat leapsâ€ privately, at Xerox PARC.

My primary source material for this part is the book,Â *â€œThe Dream* *Machine,â€* by M. Mitchell Waldrop. Iâ€™ll just refer to it as â€œTDM.â€

Iâ€™ve been dividing up this series roughly into decades, or â€œeras.â€ Part 1 focused on the 1940s and 50s. Part 2 focused on the 1960s. This part is devoted to the 1970s.

In Part 2 I covered the creation of the Advanced Research Projects Agency (ARPA) at the Department of Defense, and the IPTO (Information Processing Techniques Office, a program within ARPA focused on computer research), and the many innovative projects in which it had been engaged during the 1960s.

### Decline at the IPTO

There were signs of â€œtrouble in paradiseâ€ at the IPTO, beginning around 1967, with the acceleration of the Vietnam War. Charles Herzfeld stepped down as ARPA director that year, and was replaced by Eberhardt Rechtin. There was talk within the Department of Defense of ending ARPA altogether. Defense Secretary Robert McNamera did not like what he saw happening in the program. Money was starting to get tight. People outside of ARPA had lost track of its mission. It was recognized for producing innovative technology, but it was stuff that academics could get fascinated about. Most of it was not being translated into technologies the military could use. Its work with the academic community created political problems for it within the military ranks, because academia was viewed as being opposed to the war.

Rechtin cut a lot of programs out of ARPA. Some projects were transferred to other funders, or were spun off into their own operations. He wanted to see operational technology come out of the agency. The IPTO had some allies in John Foster at the Department of Defense Research & Engineering (DDR&E), the direct supervisor of ARPA, and Stephen Lukasik, who had had the chance to use the Whirlwind computer (which I covered in Part 1). Lukasik was very excited about the potential of computer technology. He succeeded Rechtin as ARPA director in 1970. The IPTOâ€™s budget remained protected, mainly, it seems, due to Lukasik pitching the Arpanet to his superiors as a critical technology for the military in the nuclear age (I covered the Arpanet in Part 2).

In 1970 a rule called the Mansfield Amendment, created by Democratic SenatorÂ Mike Mansfield, came into effect. It established a rule for one year mandating that all monies spent by the Defense Department had to have some stated relevance to the countryâ€™s military mission. Wikipedia says that [this rule was renewed in 1973](http://en.wikipedia.org/wiki/Mansfield_Amendment), specifically targeting ARPA. Waldrop claims it was really a round-about way of cutting defense spending, but it didnâ€™t mean anything in terms of the technology that was being developed at ARPA. Nevertheless, it had social reverberations within the agencyâ€™s working groups. In light of the controversy over the Vietnam War, it tarnished ARPAâ€™s image in the academic community, because it required all projects funded through it to justify their existence in military terms. When non-military research proposals would be sent to ARPA, military â€œrelevance statementsâ€ would be written up and slapped on them at the end of the approval process, unbeknownst to the researchers, in order to comply with the law. These relevance statements would contain descriptions of the potential, or supposed intended use of the technology for military applications. This caused embarrassing moments when students would find out about these statements through disclosures obtained through the Freedom of Information Act. They would confront ARPA-funded researchers with them. This was often the first time the researchers had seen them. Theyâ€™d be left explaining that while, yes, the military was funding their project, the focus of their research was peaceful. The relevance statements made it look otherwise. Secondly, it gave the working groups the impression that Congress was looking over their shoulder at everything they did. The sense that they were protected from interference had been pierced. This dampened enthusiasm all around, and made the recruiting of new students into ARPA projects more difficult.

ARPAâ€™s name was changed to DARPA in 1972, adding the word â€œDefenseâ€ to its name. It didnâ€™t mean anything as far as the agency was concerned, but given the academic communityâ€™s opposition to the Vietnam War, it didnâ€™t help with student recruiting.

In 1972 the consulting firm Bolt Beranek and Newman (BBN), which had built the hardware for the Arpanet, wanted to commercialize its technologyâ€“to sell it to other customers besides the Defense Department. IPTO director Larry Roberts announced in 1973 that he was leaving DARPA to work for BBNâ€™s new networking subsidiary, named Telenet. This created a big problem for Lukasik, because Roberts hadnâ€™t groomed a deputy director, as past IPTO directors had done, so that there would be someone who could jump right in and replace him when he left. It was up to Lukasik to find a replacement, only that wasnâ€™t so easy. He had created a bit of a monster within the IPTO. Every year he had pushed its budget higher, while other projects within DARPA were having their budgets cut. He tried to recruit a new director from within the IPTOâ€™s research community, but everyone he thought would be suited for it turned him down. They were enjoying their research too much. LukasikÂ came upon J.C.R. Licklider, the IPTOâ€™s founding director, as his last resort.

![](https://ars327.files.wordpress.com/2013/02/jcr-licklider1-e1360701925898.jpg?w=189&h=224)

J.C.R. â€œLickâ€ Licklider, from ARS 327

Licklider (he liked to be called â€œLickâ€), who I talked about extensively inÂ [Part 2](https://tekkie.wordpress.com/2012/02/04/a-history-lesson-on-government-rd-part-2/), had returned to MIT and ARPA in 1968. He became a tenured professor of electrical engineering. He began his own ARPA project at MIT, called the Dynamic Modeling group, in 1971. He observed that the Multics project, which I covered in Part 2, nearly overwhelmed Project MACâ€™s software engineers, and almost resulted in Multics being cancelled. The goal of his project was to create software development systems that would make complex software engineering projects more comprehensible. As part of his work he got into computer graphics, looking at how virtual entities can be represented on the screen. He also studied human-computer interaction with a graphics display.

Larry Roberts tried to find someone besides Lick to be his replacement, more as an act of mercy on him, but in the end he couldnâ€™t come up with anyone, either. HeÂ contacted Lick and asked him if heâ€™d be his replacement. He reluctantly accepted, and returned to his post as Director in 1974.

Lick became an enthusiastic supporter of Ed Feigenbaumâ€™s artificial intelligence/expert systems research. I talked a bit about Feigenbaum in Part 2. Through his guidance, Feigenbaum founded Stanfordâ€™s Heuristic Programming Project. Feigenbaumâ€™s work would become the basis for all expert systems produced during the 1980s. He also supported Feigenbaumâ€™s effort to integrate artificial intelligence (AI) into medical research, granting an Arpanet connection to Stanford. This allowed a community of researchers to collaborate on this field.

The IPTOâ€™s favored place within DARPA did not last. Lukasik didnâ€™t get along with his new superior at DDR&E, Malcom Currie. They had different philosophies about what DARPA needed to be doing.Â Waldrop wrote, â€œCurrie wanted solutions out of ARPA ***now**,*â€ and he wanted to replace Lukasik with someone of like mind.

Bob Taylor at the Xerox Palo Alto Research Center had his eye on Lukasik. He needed someone who understood how they did research, and how to talk to business executives. Taylor had been an IPTO director in the late â€™60s (I cover his tenure there in Part 2). He brought its style of computer research to Xerox, but heÂ could see that they were not set up to create products out of it.Â He thought Lukasik would be a perfect fit.Â Taylor had been recruiting many of DARPAâ€™s brightest minds into PARC, and it was a source of frustration at the agency. Nevertheless, Lukasik was intensely curious to find out just what they were up to. He came out to see what they were building. PARC was accomplishing things that Lick had only dreamed of years earlier. Lukasik was so impressed, he left DARPA to join Xerox in 1975.

![](https://i0.wp.com/upload.wikimedia.org/wikipedia/commons/thumb/6/61/George_H._Heilmeier.jpg/235px-George_H._Heilmeier.jpg)

George Heilmeier, from Wikipedia

HisÂ replacement was George Heilmeier. Waldrop characterized him as hard-nosed, someone who took an applied science approach to research. He had little patience for open-ended, basic researchâ€“the kind that had been going on at DARPA since its inception. He was a solutions man. He wanted the working groups to identify goals, where they expected their research to end up at some point in time. His emphasis was not exploration, but problem solving. Though it was jarring to everybody at first, most program directors came to accommodate it. Lick, however, was greatly dismayed that this was the new rule of the day. He thought Heilmeierâ€™s methods went against everything DARPA stood for. Lick liked the idea of finding practical applications for research, but he wanted solutions that were orders of magnitude better than older onesâ€“world changingâ€“not just short-term goals of improving old methods by ten percent. Further, he could see that micromanagement had truly entered the agency. It wasnâ€™t just a perception anymore.

Heilmeier explained in a 1991 interview,

For all the wonderful technology IPTO had sponsored, [Heilmeier insisted], it was the worst mess in the agency. And artificial intelligence was the worst mess in IPTO. â€œYou see, there was this so-called DARPA community, and a large chunk of our money went to this community. But when I looked at the so-called proposals, I thought, Wait a second; thereâ€™s nothing here. Well, Lick and I tangled professionally on this issue. He said, â€˜You donâ€™t understand. What you do is give good people the money and they go off and do good things and thatâ€™s it.â€™ I said, â€˜Lick, I understand that. And these may be good people, but for the life of me I canâ€™t tell you what theyâ€™re going to do. And I donâ€™t know whether they are going to reinvent the wheel, because thereâ€™s no discussion of the current practice and thereâ€™s no discussion of the implications, so I canâ€™t tell whether this is a wise investment for DoD or not.'â€ (TDM,Â p. 402)

Lick had a very good track record of picking research projects, using a more subjective, intuitive sense about people, and he didnâ€™t believe you could achieve the same quality of research by trying to use a more objective method of selecting people and projects.

Bob Kahn, who had joined DARPA in 1972, explained the differences between them,

[The] fact was that â€¦ both men were right. â€œThere were some things that were more conducive to Georgeâ€™s directed style,â€ says Kahn. â€œPacket radio, for example, and maybe the Internet. But speech understanding was not amenable. In fact, almost none of AI fit in. The problem was that George kept looking for a kind of road map to the field of AI. He wanted to know what was going to happen, on schedule, into the future, to make the field a reality. And he thought it was quite reasonable to ask for that road map, because he had no idea how hard it would be to produce. Suppose you were Lewis and Clark exploring the West, where you had no idea what you were going to encounter, and people wanted to know exactly what routes you were going to take, where you would camp, and what you would do out there. Well, this was just not an engineering job, where you could work out the whole plan. So George was looking for something that Lick couldnâ€™t provide.â€

Unfortunately, when Lick tried to explain that to his boss, it was a bit like Seymour Papertâ€™s or Alan Kayâ€™s trying to explain exploratory education to a back-to-basics hard-liner. â€œIPTO really didnâ€™t have a program-management structure,â€ declares Heilmeier. â€œThey had a financial management structure, and they had a cheering section.â€ (TDM,Â p. 403)

Interestingly, I found this [article in EETimes](http://www.eetimes.com/electronics-news/4402443/George-Heilmeier--Inventor-of-LCD-did-so-much-more) that saw this from a very different perspective, saying that what DARPA had been running was a â€œgood-old-boys networkâ€ (and itâ€™s difficult to see how itâ€™s not implicating Lick in this), and that Heilmeier snapped them into shape.

Lick tried to see it from Heilmeierâ€™s perspective, that researchers should not see applications as a threat to basic research, and that they should make common cause with engineers, to bring their work into the world of people who will use their technology. He thought this could provide an avenue where researchers could receive feedback that would be valuable for their work, and would hopefully soften the antagonism that was being directed at open-ended research. Meanwhile, heâ€™d quietly try to keep Heilmeier from destroying what he had worked so hard to achieve.

Heilmeier seemed to agree with this approach. He issued some challenges to the IPTO working groups of technical problems he saw needed to be solved. He said to them, â€œLook, if some of you guys would sign up for these challenges I can justify more fundamental work in AI.â€ Some of them did just as he asked, and got to work on the challenges.

Lick declared in 1975 that the Arpanet, a project which began at DARPA in 1967, was fully operational, and handed control of the network over to the Defense Communications Agency.

Lick had to kill funding for some projects, which was never easy for him. One in particular was Doug Engelbartâ€™s NLS project at Stanford Research Institute. I talked about this project in [Part 2](https://tekkie.wordpress.com/2012/02/04/a-history-lesson-on-government-rd-part-2/), and what ended up happening with it. A lot of Engelbartâ€™s talent had left to pursue the development of personal computing at Xerox PARC. From Lickâ€™s and DARPAâ€™s perspective, he just wasnâ€™t able to innovate the way he had before. From Engelbartâ€™s perspective, Lick had been captured by the hype around AI, and just wasnâ€™t able to see the good he was doing. It was a sad parting of the ways for both of them. (Source: [Nerd TV interview with Doug Engelbart](https://archive.org/details/nerdtv011))

Heilmeier was pleased with the changes at the IPTO. They had â€œgotten with the program,â€ and he thought they were producing good results. For Lick, however, the change was exhausting, and depressing. He left DARPA for good in late 1975, and returned to MIT, where his spirit soared again. This didnâ€™t mean that he left computing behind. He came up with his own projects, and he encouraged students to play with computers, as he loved to do.

Bob Kahn at DARPA asked an important question. He agreed with Heilmeierâ€™s â€œwire brushingâ€ of the agency; that some projects had become self-indulgent, but he cautioned against â€œtoo much of a good thingâ€ the other way.

ARPAâ€™s current obsession with â€œrelevanceâ€ had come dangerously close to destroying what made the agency so special. Remember, says Kahn, when it came to basic computer researchâ€“the kind of high-risk, high-payoff work that might not mature for a decade or moreâ€“ARPA was almost the only game in town. The computer industry itself was oriented much more toward products and services, he says, which meant that â€œthere was actually very little research going on that was as innovative as ARPAâ€™s.â€ And while there were certainly some shining exceptions to that ruleâ€“notably [Xerox] PARC, IBM and [AT&T] Bell Labsâ€“â€œmany of the leading scientists and researchers couldnâ€™t be supported that way. So if universities didnâ€™t do basic research, where would industry get its trained people?â€

Yet it was ARPAâ€™s basic research that was getting cut. â€œThe budget for basic R&D was only about one third of what it was before Lick came in,â€ says Kahn. â€œMorale in the whole computer-science community was very low.â€ (TDM,Â p. 417)

Kahn pursued a research initiative, anticipating the future of VLSI (Very-Large-Scale Integration) design and fabrication for microchips, in order to try to revive the basic research culture at the IPTO. He won approval for it in 1977. Through it, methods were developed for creating computer languages for designing VLSI chips.Â Kahn was also the co-developer of TCP/IP with Vint Cerf at DARPA, the protocol that would create the internet.

Heilmeier left DARPA in 1977, and was replaced by Bob Fossum, who had a management style that was more amenable to basic research. The question was, though, â€œOkay. This is good, but what about theÂ *next* director?â€ It had been common practice for people at DARPA to cycle out of administrative positions every few years. Was it too good to last?

Kahn began the [Strategic Computing Initiative](http://en.wikipedia.org/wiki/Strategic_Computing_Initiative) in 1983, a $1 billion program (about $2.3 billion in todayâ€™s money) to continue his work on advancing computer hardware design, and to advance research in artificial intelligence.Â Fossum was replaced by Robert Cooper that same year. Cooper took Kahnâ€™s research goals and reimplemented them as an agency-wide program, giving every part of DARPA a piece of it. According to Waldrop, Kahn was disgusted by this, and took it as his cue to leave.

The era of revolutionary research in computing at the agency seems to come to an end at this point.

Hereâ€™s a video talking about what else was going on at DARPA during the period Iâ€™ve just covered:

https://www.youtube.com/embed/2X2FNru31nM?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en&autohide=2&wmode=transparent

### Lickâ€™s vision of the future

He would happily sit for hours, spinning visions of graphical computing, digital libraries, on-line banking and E-commerce, software that would live on the network and move wherever it was needed, a mass migration of government, commerce, entertainment, and daily life into the on-line worldâ€“possibilities that were just mind-blowing in the 1970s. (TDM,Â p. 413)

Lick had long been a futurist, a very reliable one. In a book published in 1979,Â *â€œThe Computer Age: A Twenty-Year View,â€* he looked into the future, to the year 2000, about what he could see happeningâ€“if he thought optimisticallyâ€“with a nationwide digital network that he called â€œthe Multinet.â€ The term â€œInternetâ€ was not in wide use yet, though work on TCP/IP, what Lick called the â€œKahn-Cerf internetworking protocol,â€ had been in progress for several years. The internet wouldnâ€™t come into being for another 4 years.

â€œWaveguides, optical fibers, rooftop satellite antennae, and coaxial cables, provide abundant bandwidth and inexpensive digital transmission both locally and over long distances. Computer consoles with good graphic display and speech input and output have become almost as common as television sets.â€

Great. But what would all those gadgets add up to, Lick wondered, other than a bigger pile of gadgets? Well, he said, if we continued to be optimists and assumed that all this technology was connected so that the bits flowed freely, then it might actually add up to an electronicÂ *commons* open to all, as â€œthe main and essential medium of informational interaction for governments, institutions, corporations, and individuals.â€ Indeed, he went on, looking back from the imagined viewpoint of the year 2000, â€œ[the electronic commons] has supplanted the postal system for letters, the dial-tone phone system for conversations and tele-conferences, stand-alone batch processing and time-sharing systems for computation, and most filing cabinets, microfilm repositories, document rooms and libraries for information storage and retrieval.â€

â€¦

The Multinet would permeate society, Lick wrote, thus achieving the old MIT dream of an information utility, as updated for the decentralized network age: â€œmany people work at home, interacting with coworkers and clients through the Multinet, and many business offices (and some classrooms) are little more than organized interconnections of such home workers and their computers. People shop through the Multinet, using its cable television and electronic funds transfer functions, and a few receive delivery of small items through adjacent pneumatic tube networks . . . Routine shopping and appointment scheduling are generally handled by private-secretary-like programs called OLIVERs which know their mastersâ€™ needs. Indeed, the Multinet handles scheduling of almost everything schedulable. For example, it eliminates waiting to be seated at restaurants.â€ Thanks to ironclad guarantees of privacy and security, Lick added, the Multinet would likewise offer on-line banking, on-line stock-market trading, on-line tax paymentâ€“the works.

In short, Lick wrote, the Multinet would encompass essentially everything having to do with information. It would function as a network of networks that embraced every method of digital communication imaginable, from packet radio to fiber opticsâ€“and then bound them all together through the magic of the Kahn-Cerf internetworking protocol, or something very much like it.

â€¦

Lick predicted its mode of operation would be â€œone featuring cooperation, sharing, meetings of minds across space and time in a context of responsive programs and readily available information.â€ The Multinet would be the worldwide embodiment of equality, community, and freedom.

*If*, that is, the Multinet ever came to be. (TDM,Â p. 413)

He tended to be pessimistic that this all would come true. With the world as it was, he thought a more tightly controlled scenario was more likely, one where the Multinet did not get off the ground. He thought big technology companies would not get into networking, as it would invite government regulation. Communications companies like AT&T would see it as a threat to their business. Government, he thought, would not want to share information, and would rather use computer technology to keep proprietary files on people and corporations. HeÂ thought the only way his positive vision would come to pass was if a consensus of hundreds of thousands, or millions of people came about which agreed that an open Multinet was desirable. He felt this would require leadership from someone with a vision that agreed with this idea.

At this time there were packet switching network options offered by various technology companies, including IBM, Digital Equipment Corp. (DEC), and Xerox, but none of them offered openness. In fact, business customers wanted closed networks. They feared openness, due to possibilities of security leaks and industrial espionage. Things were not looking up for this â€œmarketplaceâ€ vision of a future network, even in academia. Michael Dertouzos, who led the Laboratory of Computer Science (LCS) at MIT (formerly Project MAC), was very interested in Lickâ€™s vision, but complained that his fellow academics in the program were not. In fact they were openly hostile to it. It felt too outlandish to them.

Microcomputers had taken off in 1975, with the introduction of the [MITS Altair](http://en.wikipedia.org/wiki/MITS_Altair), created by electrical engineer Ed Roberts. (This was also the launching point for a small venture created by Bill Gates and Paul Allen, called â€œMicro Soft,â€ with their first product, a version of the Basic programming language for the Altair.) Lick bought an [IBM PC](http://en.wikipedia.org/wiki/IBM_PC) in the early 1980s, â€œbut it never had the resources to do what he wanted,â€ said his son, Tracy.

Yes, Lick knew, these talented little micros had been good enough to reinvent the computer in the public mind, which was no small thing. But so far, at least, they had shown people only the faintest hint of what was possible. Before his vision of a free and open information commons could be a reality, the computer would have to be reinvented several more times yet, becoming not just an instrument for individual empowerment but a communication device, an expressive medium, and, ultimately, a window into on-line cyberspace.

In short, the mass market would have to give the public something much closer to the system that had been created a decade before at Xerox PARC. (TDM,Â p. 437)

### Personal computing

The major sources I used for this part of the story were Alan Kayâ€™s retrospective on his days at Xerox, calledÂ [*â€œThe Early History of Smalltalkâ€*](http://gagne.homedns.org/~tgagne/contrib/EarlyHistoryST.html)Â (which I will callÂ â€œTEHSâ€ hereafter), andÂ [*â€œThe Alto and Ethernet Software,â€*](https://www.researchgate.net/publication/2688342_Personal_Distributed_Computing_The_Alto_and_Ethernet_Software) by Butler Lampson (which I will call â€œTAESâ€).

Parallel to the events I describe at DARPA came a modern notion of personal computing. The vision of a personal computer existed at DARPA, but as Iâ€™ll describe, the concept we would recognize today was developed solely in the private sector, using knowledge and research methods that had been developed previously through DARPA funding.

![](https://tekkie.wordpress.com/wp-content/uploads/2009/06/alan_kay2.jpg?w=187&h=240)

Alan Kay, from Wikipedia

The idea of a computer that individuals could buy and own had been around since 1961. Wes Clark had that vision with the [LINC](http://history-computer.com/ModernComputer/Personal/LINC.html) computer he invented that year at MIT. Clark invited others to become a part of that vision. One of them was Bob Taylor. What got in the way of this idea becoming something that less technical people could use was the large and expensive components that were needed to create sufficiently powerful machines, and some sense among developers about just how ordinary people would use computers. Most of the aspects that make up personal computing were invented on larger machines, and were later miniaturized, watered down in sophistication, and incorporated into microcomputers from the mid-1970s into the 1990s and beyond.

Most people in our society who are familiar with personal computers think the idea began with Steve Jobs and Steve Wozniak. The more knowledgeable might intone, and give credit to Ed Roberts at MITS, and Bill Gates. These people had their own ideas about what personal computers would be, and what they would represent to the world, but in my estimation, the idea of personal computing that we would recognize today really began with Alan Kay, a post-graduate student with a background in mathematics and molecular biology, who had become enrolled in the IPTOâ€™s computer science program at the University of Utah (which I talk a bit about in [Part 2](https://tekkie.wordpress.com/2012/02/04/a-history-lesson-on-government-rd-part-2/)), in the late 1960s. A good deal of credit has to be given to Doug Engelbart as well, who was doing research during the 1960s at the Stanford Research Institute, funded by ARPA/IPTO, NASA, and the Air Force, on how to improve group knowledge processes using computers. He did not pursue personal computing, but he was the one who came up with the idea for a point-and-click interface, combining graphics and text, using a device he and his team had invented called a â€œmouse.â€ He also created the first system that enabled linked documents, which foreshadowed the web weâ€™ve known for the last 20 years, and enabled collaborative computing with teleconferencing. Describing his work this way does not do it justice, but itâ€™ll suffice for this discussion.

![](https://i0.wp.com/www.ibiblio.org/pioneers/images/pics/engelbart.gif)

![](https://i0.wp.com/upload.wikimedia.org/wikipedia/commons/5/5c/Ivan_Sutherland_at_CHM.jpg)

![](https://tekkie.wordpress.com/wp-content/uploads/2014/06/seymour-papert.gif?w=176&h=211)

Doug Engelbart,
from ibiblio.com

Ivan Sutherland,
from Wikipedia

Seymour Papert, from MIT

![](https://i0.wp.com/static.flickr.com/4/4958562_df106cff36.jpg)

Flexâ€™s â€œself-portrait,â€ from lurvely.com

Kay began developing his ideas about personal computing in 1968. He had started on his first proof-of-concept desktop machine, called Flex, a year earlier. He was aware of Mooreâ€™s Law (from [Gordon Moore](http://en.wikipedia.org/wiki/Gordon_Moore)), a prediction that as time passed, more and more transistors would fit in the same size space of silicon. The implication of this is that more computing functionality could fit in a smaller space, which would thereby allow machines which filled up a room at the time to become smaller as time passed. As a graduate student, he imagined miniaturized technology that seemed unfathomable to other computer scientists of his day. For example, a hard drive as small as the crook of your finger. Back then, a hard drive was the size of a floor cabinet, or medium-sized refrigerator, and was typically used with mainframes that took up the space of a room.

Kayâ€™s ideas about what personal computing could be were heavily influenced by Doug Engelbart, [Ivan Sutherland](http://en.wikipedia.org/wiki/Ivan_Sutherland) (from his [Sketchpad](http://en.wikipedia.org/wiki/Sketchpad) project), Tom Ellis and Gabriel Groner at RAND Corp. (from a system called [GRAIL](http://youtu.be/n60NEAhhgG8)), and Seymour Papert, Wally Feurzig, and Cynthia Soloman at BBN with their work with children and [Logo](http://en.wikipedia.org/wiki/Logo_(programming_language)), among many others. (Source: TEHS)

Engelbart thought of computing as a â€œvehicleâ€ for thinking about, and sharing information, and developing group knowledge processes. Kay got a profound sense of the personal computerâ€™s place in the world from Papertâ€™s work with children using Logo. He realized that personal computers could not just be a vehicle, but a new medium.

People can get confused about this concept. Kay wasnâ€™t thinking that personal computers would allow people to use and manipulate text, images, audio, and video (movies and TV)â€“what most of us think of as â€œmediaâ€â€“for the sake of doing so. He wasnâ€™t thinking that they would be a new way to store, transmit, and presentÂ *old* media, and the ideas they expressed most easily. Rather, they would allow people to explore a wholeÂ *new* category of ideas and expression that the other forms of media did not express as easily, if at all. Itâ€™s not that the old media couldnâ€™t be part of the new, but they would be represented as models, as part of a knowledge system, and operate under a personâ€™s control at whatever grain would facilitate what they wanted to understand.

Butler Lampson described the concept this way:

Kay was pursuing a different path to Lickliderâ€™s man-computer symbiosis: the computerâ€™s ability to simulate or model any system, any possible world, whose behavior can be precisely defined. And he wanted his machine to be small, cheap, and easy for nonprofessionals to use. (TAES, p. 2)

Kay could see from Papertâ€™s work that using computers as an interactive medium could enable children to understand subjects that would otherwise have been difficult, if not impossible to grasp at their age, particularly aspects of mathematics and science.

### Xerox PARC

Xerox enters the story in 1970. They had a different set of priorities from Kayâ€™s, and itâ€™s interesting to note that even though this was true, they were willing to accommodate not only his priorities, but those of other researchers they brought in.

Xerox was concerned that as the photocopier market grew, competitors would come into the space, and Xerox didnâ€™t want to put all their â€œeggsâ€ in it. They thought computers would be a good way for them to diversify their product line. Their primary goal was toâ€¦

â€¦develop the â€œarchitecture of informationâ€ and establish the technical foundation for electronic office systems that could become products in the 1980s. It seemed likely that copiers would no longer be a high-growth business by that time, and that electronics would begin to have a major effect on office sysÂ­tems, the firmâ€™s major business. Xerox was a large and prosperous company with a strong commitment to basic research and a clear need for new technology in this area. (TAES, p. 2)

Xerox wanted to site their research facility near a community of computer research. They looked at a few places around the country, and ultimately decided on Palo Alto, CA, since it was near Berkeley and Stanford universities, which were centers of computer research. They wanted to bring in a research director who was familiar with the field, and would be respected by the research community. The right person for the job was not obvious. When they talked to computer researchers of the time, all paths eventually led back to Bob Taylor, who had been an ARPA/IPTO director in the late 1960s. The thing was he had no computer science research of his own under his belt. His background was in psychoacoustics, a field of psychology (though Taylor thinks of itÂ as applied physics). What made him the right candidate in Xeroxâ€™s eyes was that everyone who was anyone in the field Xerox wanted to explore knew and respected him. So they invited Taylor in to assist in setting up their research group.Â Â Thus was born the Xerox Palo Alto Research Center (PARC). It was totally funded by Xerox. Taylor was not officially given the job as PARCâ€™s director, because of his lack of research background, but he was allowed to run the facility the same way that the IPTO had conducted computer research. You could say that PARC during the 1970s was â€œthe ARPA way done privately.â€

One of the research techniques used at ARPA and Xerox PARC was to anticipate the speed and memory capacities ofÂ *future*Â computers that would be in wide use. Engineers who were part of the research teams would either find hardware that fit these anticipated specifications, or would build their own, and then see what software they could develop on it that was significantly better in some capacity than the systems that were available in their present. As one can surmise, this was an expensive thing to do. In order to exceed the capacity of the machines that were in wide use, one had to not think about what was most economical, but rather go for the hardware that was only used by a relative few, if anyone was using it at all, because it would be prohibitively expensive for most to acquire. Butler Lampson described this with Xerox PARCâ€™s Alto research system:

The Alto system was affected not only by the ideas its builders had about what kind of system to build, but also by their ideas about how to do computer systems research. In particular, we thought that it is important to predict the evolution of hardware technology, and start working with a new kind of system five to ten years before it becomes feasible as a commercial product.

â€¦

Our insistence on workÂ­ing with tomorrowâ€™s hardware accounts for many of the differences between the Alto system and the early personal computers that were coming into existence at the same time. (TAES, p. 3)

(To get an idea of what Lampson was talking about in that last sentence, I encourage people look at another post I wrote a while back, called [â€œTriumph of the Nerds.â€](https://tekkie.wordpress.com/2007/04/02/triumph-of-the-nerds/))

Taylor hired a bunch of people from the failing Berkeley Computer Corp., which was started by Butler Lampson, along with students that had joined him as part of Project Genie. Among the people Taylor brought in were Chuck Thacker, Charles Simonyi, and Peter Deutsch. I talk a little about Project Genie in [Part 2](https://tekkie.wordpress.com/2012/02/04/a-history-lesson-on-government-rd-part-2/). He also hired many people from ARPAâ€™s IPTO projects, including Ed McCreight from Carnegie Mellon University, and some of the best talent from Engelbartâ€™s NLS project at the Stanford Research Institute, such asÂ Bill English. Jerry Elkind hired people from BBN, which had been an ARPA contractor, including Danny Bobrow, Warren Teitelman, and Bert Sutherland (Ivan Sutherlandâ€™s older brother). Another major â€œgetâ€ was hiring Allen Newell from CMU as a consultant. A couple of Newellâ€™s students, Stuart Card and Tom Moran, came to PARC and pioneered the field we now know as Human-Computer Interaction.

As Larry Tesler put it, Xerox told the researchers, â€œGo create the new world. We donâ€™t understand it. Here are people who have a lot of ideas, and tremendous talent.â€ Adele Goldberg said of PARC, â€œPeople came there specifically to work on 5-year programs that were their dreams.â€ (Source: Robert X. Cringelyâ€™s documentary, *â€œTriumph of the Nerdsâ€*) In a recent interview, which Iâ€™ll refer to at the end of this post, she said that PARC invited researchers in to work on anything that they thought in 5 years could have an impact on the company.

Alan Kay came to work at PARC in 1970, started the Learning Research Group (LRG), and got them thinking about what would come to be known as â€œportable computers,â€ though Kay called them â€œKiddieKomps.â€ They also got working on font technology.

In 1972 Kay committed his thoughts on personal computing to paper in a document called, [â€œA Personal Computer for Children of All Ages.â€](http://www.mprove.de/diplom/gui/Kay72a.pdf)Â In it he described a conceptual model he called a â€œDynabook,â€ or â€œdynamic book.â€Â Iâ€™ll quote from a blog post I wrote in 2006, called, [â€œGreat moments in modern computer history,â€](https://tekkie.wordpress.com/2006/08/22/great-moments-in-modern-computer-history/) as it summarizes what I mean to get across about this:

Kay envisioned the Dynabook as a portable computer, 9â€³ x 12â€³ x 3/4â€³, about the size of a modern laptop, with its own battery, and would weigh less than 4 pounds. In fact he said, â€œThe size should be no larger than a notebook.â€ He envisioned that it would use removable media for file storage (about 1 MB in size, he said), that it might have a keyboard, and that it would record and play audio files, in addition to displaying text. He said that if no physical keyboard came with the unit, a software keyboard could be brought up, and the screen could be made touch-sensitive so that the user could just type on the screen. â€¦ Oh, and he made a wild guess that it would cost no more than $500 to the consumer.

He said, â€œThe owner will be able to maintain and edit his own files of text and programs, when and where he chooses.â€

â€¦

He envisioned thatÂ the DynabookÂ would be able to â€œdockâ€ with a larger computer system at work. The user could download data, and recharge its battery while hooked up. He figured the transfer rate would be 300 Kbps.

â€¦

![](https://i0.wp.com/history-computer.com/ModernComputer/Personal/images/dynabook1.jpg)

Mock up of the Dynabook,
fromÂ history-computer.com

He imagined the Dynabook being connected to an â€œinformation utility,â€ like what was called at the time â€œthe ARPA network,â€ which later came to be called the Internet. He predicted thisÂ would open up online access toÂ schools and libraries of information, â€œstoresâ€ (a.k.a. e-commerce sites), and would bring â€œbillboardsâ€ (a.k.a. web ads) to the user. I LOVE this quote: â€œOne can imagine one of the first programs an owner will write is a filter to eliminate advertising!â€

â€¦

Another forward-looking concept he imagined is that it might have a flat-panel plasmaÂ display. He wasnâ€™t sure if this would work, since it would draw a lot of power, but he thought it was worth trying. â€¦ He thought an LCD flat-panel screen was another good option to consider.

Kay thought it essential that the machine make it possible to use different fonts. He and fellow researchers had already done some experiments with font technology, and he showed some examples of their results in his paper.

![](https://tekkie.wordpress.com/wp-content/uploads/2016/02/children-using-the-dynabook-2.jpg?w=150&h=100)

Alan Kayâ€™s conception of children using the Dynabook

He doesnâ€™t elaborate on this, but he hints at a graphical interface for the device. He describes in spots how the user can create and save â€œdynamic graphics.â€ In a scenario he illustrates in the paper, two children are playing a game like Space War on the Dynabook, involving graphics animation,Â experimenting with concepts of gravity. This scenario lays downÂ the concept of it being a learning machine, and one thatâ€™s easy enough for children to manipulate through a programming language.

He also imagined that Dynabooks would be able to communicate with each other wirelessly, [peer-to-peer](http://en.wikipedia.org/wiki/Peer-to-peer_networking), so that groups of students would be able to easily work on projects together, without needing an external network.

Working with Dan Ingalls, an electrical engineer with a background in physics, Diana Merry, and colleagues, the LRG began development of a system called â€œSmalltalkâ€ in 1972. This was a first effort to create the Dynabook in software. It would come to formalize another of Kayâ€™s concepts, of virtual objects in a computer. The idea was that entities (visual and non-visual) could be fashioned by the person using a computer, which are as versatile as tools that are used in the real world, and can be used in any combination at any time to accomplish tasks that may not have been evident when they were created.

Computer programming was an important part of Kayâ€™s concept of how people would interact with this medium, though he developedÂ doubts about it. What he really wanted was some way that people could build models in the computer. Programming just seemed to be a good way to do it at the time.

Objects could be networked together via. a concept called â€œmessage passing,â€ with the goal of having them work together for some purpose. As Smalltalk was developed over 8 years, this idea would come to include everything from the desktop interface, to windows, to text characters, to buttons, to menus, to â€œpaintâ€ brushes, to drawing tools, to icons, and more. His idea of overlapping windows came from his desire to allow people to work on several projects at the same time, allowing them to make the most of limited screen space. (source:Â TEHS)

### â€œThe best way to predict the future is to invent itâ€

The graphical interface he and his team developed for Smalltalk was motivated by educational research on children, which had surmised that they have a strong visual sense, and that they relate to manipulating visual objects better than explicitly manipulating symbols, as older interactive computer systems had insisted upon. A key phrase in the paragraph below is, â€œdoing with imagesÂ *makes* symbols,â€ that is, symbols in our own minds, and, if you will, in the â€œmindâ€ of the computer. His concept of â€œdoing with imagesâ€ was much more expansive and varied than has typically been allowed on computers consumers have used.

All of the elements eventually used in the Smalltalk user interface were already to be found in the sixtiesâ€“as different ways to access and invoke the functionality provided by an interactive system. The two major centers were Lincoln Labs [at MIT] and RAND Corpâ€“both ARPA funded. The big shift that consolidated these ideas into a powerful theory and long-lived examples came because the LRG focus was on children. Hence we were thinking about learning as being one of the main effects we wanted to have happen. Early on, this led to a 90 degree rotation of the purpose of the user interface from â€œaccess to functionalityâ€ to â€œenvironment in which users learn by doingâ€. This new stance could now respond to the echos of Montessori and Dewey, particularly the former, and got me, on rereading Jerome Bruner, to think beyond the childrenâ€™s curriculum to a â€œcurriculum of the user interface.â€

The particular aim of LRG was to find the equivalent of writingâ€“that is learning and thinking by doing in a mediumâ€“our new â€œpocket universeâ€. For various reasons I had settled on â€œiconic programmingâ€ as the way to achieve this, drawing on the iconic representations used by many ARPA projects in the sixties. My friend Nicolas Negroponte, an architect, was extremely interested in embedding the new computer magic in familiar surroundings. I had quite a bit of theatrical experience in a past life, and remembered Coleridgeâ€™s adage that â€œpeople attend â€˜bad theatreâ€™ hoping to forget, people attend â€˜good theatreâ€™Â *aching to remember*â€œ. In other words, it is the ability to evoke the audienceâ€™s own intelligence and experiences that makes theatre work.

Putting all this together, we want an apparently free environment in which exploration causes desired sequences to happen (Montessori); one that allows kinesthetic, iconic, and symbolic learningâ€“*â€œdoing with images makes symbolsâ€*Â (Piaget & Bruner); the user is never trapped in a mode (GRAIL); the magic is embedded in the familiar (Negroponte); and which acts as a magnifying mirror for the userâ€™s own intelligence (Coleridge). (source: TEHS)

https://www.youtube.com/embed/AuXCc7WSczM?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en&autohide=2&wmode=transparent

*A demonstration of Smalltalk-80*

![](https://i0.wp.com/upload.wikimedia.org/wikipedia/commons/2/2f/Dan_Ingalls.jpg)

![](https://i0.wp.com/m.c.lnkd.licdn.com/mpr/pub/image-u8x1umFX4RFEfRET0dkRMUYjEbUH0ESgu8Ku9pnBETmj0NxNu8xu9nEXEhNV0kpnXk1J/diana-merry-shapiro.jpg)

![](https://tekkie.wordpress.com/wp-content/uploads/2014/06/chuck-thacker.jpg?w=125&h=117)

![](https://tekkie.wordpress.com/wp-content/uploads/2014/06/butler-lampson.jpg?w=94&h=117)

Dan Ingalls,
from Wikipedia

Diana-Merry Shapiro (not pictured)

Chuck Thacker,
from the ACM

Butler Lampson,
from MIT

Chuck Thacker, who has a background in physics and designing computer hardware, and a small team he assembled, created the first â€œInterim Dynabook,â€ as Kay called it, at PARCâ€™s Computer Science Lab (CSL) in 1973. It was named â€œBilbo.â€ It was not a handheld device, but more like the size of a desk, perhaps connected to a larger cabinet of electronics. It had a monitor with a bitmap display of 606 x 808 pixels (which gave it the profile of an 8-1/2â€³ x 11â€³ sheet of paper), and 128 kilobytes of memory. Smalltalk was brought over to it from its original â€œhome,â€ aÂ [Data General Nova computer](http://en.wikipedia.org/wiki/Data_General_Nova).

Thackerâ€™s team created the first Alto models (named after Palo Alto) shortly thereafter. Butler Lampson, a computer scientist with a background in physics, and electrical engineering, led a team which created the operating system for it. The Alto computer fit under a desk. It was the size of a small refrigerator. The keyboard, mouse, and display sat on top of the desk. It had the same size display, and internal memory as â€œBilbo,â€ ran at about 6 Mhz, and had a removable hard drive that stored 2.5 MB per disk. (source: [History of Computers](https://history-computer.com/technology/xerox-alto-guide/))

As the years passed, CSL would develop bigger, more powerful machines, with code names Dolphin, and Dorado.

![](https://i0.wp.com/farm3.staticflickr.com/2656/5711666761_6bdc0a5419_z.jpg)

Adele Goldberg, from PC Forum

Beginning in 1973, Adele Goldberg, who had a background in mathematics and information systems, and Alan Kay tried Smalltalk out on students from ages 12-15, who volunteered to take programming courses from them, and to come up with their own ideas for things to try out on it. Goldberg developed software design schema and curricular materials for these courses, and helped guide the education process as they got results.

They had some success with an approach developed by Goldberg where they had the students build more and more sophisticated models with graphical objects. They thought they were building the skill of students up to more sophisticated approaches to computing, and in some ways they were, though the influence these lessons were having on the studentsâ€™ conception of computing was not as broad as they thought. Kay realized after teaching programming to some adult students that they could only get so far before they ran into a â€œliteratureâ€ barrier. The same had been true with the teen and pre-teen students they had taught earlier, it turned out. From Kayâ€™s description, the way Iâ€™d summarize the problem is that they required background knowledge in organizing their ideas, and they needed practice in doing this.

Kay said in retrospect that literature renders ideas. Any medium needs literature in order to be powerful. Literatureâ€™s purposeÂ is to provide a body of ideas that can be discussed in the medium. (Source: TEHS) Without this literary background,Â the students were unable to write about, or discuss the more sophisticated ideas through programming code.Â No matter how good a tool or instrument is, whatâ€™s produced by the people using it can only be as good as the ideas they use in fashioning the product. Likewise, the quality of what is written in text or notation by an author or composer, or produced in sound by a musician, or imagery and sound for a movie, is only as good as a) the skill of the creators, b) the outlook they have on what they are expressing, and c) their knowledge, which they can apply to the effort.

### Reconsidering the Dynabook

There came a â€œdividing pointâ€ at PARC in 1975. Kay met with other members of the LRG and discussed â€œstarting over.â€ He could see that the developed ideas of Smalltalk were taking them away from the educational goals he set out to accomplish. Professional considerations had started to take hold with the group, though, and they saw potential with Smalltalk. The majority of the group wanted to stick with it.Â His argument for â€œstarting overâ€ with the educational project did not win out. He said of this point in time that while he disagreed with the decision of his colleagues, he held no ill will towards them for it. He knew them to be wonderful people. The sense I get from reading Kayâ€™s account of this history is they saw potential perhaps in creating more sophisticated user environments that professionals would be interested in using. I infer this from the projects that PARC engaged in with Smalltalk thereafter.

Kay turned his focus to a new project, as if to pick up again his â€œKiddieKompsâ€ idea. He designed a computer he called NoteTaker.Â Adele Goldberg said Kayâ€™s purpose in doing this was to develop a proof of concept for the Dynabookâ€™s hardware (see the interview with her at the end of this post). While Kay went off in this direction, Goldberg took over management of the Smalltalk project. The educational program at PARC faded away in 1976. (Source: TEHS)

The original concept for NoteTaker was a laptop design, with what Kay called a â€œtab mouse,â€ a physical control that was small, mounted on the computer, yet agile enough to allow a person using it to move a cursor around a graphical interface. This did not end up on the machine, but NoteTaker was made useable with a keyboard, mouse, and a touch screen, and it had stereo sound output. (Source: [â€œJoining the Mac Group,â€](http://www.folklore.org/StoryView.py?project=Macintosh&story=Joining_the_Mac_Group.txt&sortOrder=Sort+by+Date&characters=Jerry+Manock&showcomments=1)Â by Bruce Horn)

Once Smalltalk-76 was done (each version of Smalltalk was named by the year it was completed), Dan Ingalls and Ted Kaehler ported it to the NoteTaker, and by around 1978 Kay had created the first â€œluggableâ€ portable computer. Kay recalls it using three Intel 8086 processors, that it had 256 kilobytes of memory, and was able to run on batteries. It wasnâ€™t a laptop (it was too big and heavy), but it could fit on a desktop. At first glance, it looks similar to the [Osborne 1](http://en.wikipedia.org/wiki/Osborne_1), the first commercially available portable computer, and thereâ€™s a reason for that. The Osborneâ€™s case design was based on NoteTaker.

To put it through its paces, a team from PARC took the NoteTaker on an airplane trip, â€œrunning an object-oriented system with a windowed interface at 35,000 feet.â€

Kay lamented that there wasnâ€™t enough corporate will to use their own know-how to create better hardware for it (Kay considered the Intel processors barely sufficient), much less turn it into a commercial product.

![](https://i0.wp.com/archive.computerhistory.org/resources/physical-object/xerox/X748-86A.1.lg.jpg)

The NoteTaker, from the Computer History Museum

Looking back on the experience, Kay was dismayed to see that PARC had pushed aside his educational goals, and had co-opted Smalltalk as a purely professionalâ€™s tool. The technologies that could have made his original Dynabook vision a physical reality were coming into being at just this time, but there was no will at Xerox to make it into an actual product. (source: TEHS)

HeÂ has pursued development of his educational ideas ever since. He sees computers in the same way that a few in the Middle Ages saw the invention of the printing press, enabling new ways of interacting and thinking, to, as Licklider would have said, allow people to â€œthink as no one has ever thought before.â€

### Developing the office of the future

![](https://pbs.twimg.com/profile_images/256841496/Larry_2009_square_400x400.jpg)

![](https://s3.amazonaws.com/startupgenome-avatars/people/timothy-mott.png)

![](https://tekkie.wordpress.com/wp-content/uploads/2014/06/93ewis17.jpg?w=93&h=100)

From left to right: Larry Tesler, from Twitter.com; Timothy Mott, from startupgenome.co; Charles Simonyi

A separate project at PARC also got started in the early 1970s, called POLOS (the Parc On-Line Office System), in the System Science Lab (SSL). The goal there was to develop a networked computer office system.

**Edit** 5/31/2016: I had suggested in the way I wrote the following paragraph that Larry Tesler had invented cut, copy, and paste actions in digital text editing. Upon further review, I think that historical interpretation is wrong. Doug Engelbart had copy and paste (and probably a â€œcutâ€ action) in NLS, and there may have been earlier incarnations of it.

One of the first projects in this effort was a word processor called Gypsy, designed and implemented by Larry Tesler and Timothy Mott, both computer scientists. (Source: TAES) The unique thing about Gypsy was that Tesler tried to make it â€œmodeless.â€ Its behavior was like our modern day word processors, where you always have a cursor on the screen, and all actions mean the same thing at all times. Wherever the cursor is, thatâ€™s where text is entered. He also invented drag selection/highlighting with a mouse, and which was incorporated into the a cut, copy, and paste process. This has been a familiar feature whenever we work with digital text.

Later, Charles Simonyi, an electrical engineer, and Butler Lampson began work on the first What You See Is What You Get (WYSIWYG) text processor, called Bravo. They developed their first version in 1974. It had its own graphical interface, allowed the use of fonts, and basic document elements, like italics and boldface, but it worked in â€œmodes.â€ The person using it either used the computerâ€™s keyboard to edit text, or to issue commands to manipulate text. The keyboard did something different depending on which mode was operating at any point in time. This was followed by BravoX, which was a â€œmodelessâ€ version. BravoX had a menu system, which allowed the use of a mouse for executing commands, making it more like what weâ€™d recognize as a word processor today. (source: [Wikipedia](http://en.wikipedia.org/wiki/Bravo_(software))) Each team was trying out different capabilities of digital text, and trying to see how people worked with a text system most productively.

![](https://tekkie.wordpress.com/wp-content/uploads/2014/06/andrew-birrell.jpg?w=100&h=100)

![](https://www.linkedin.com/mpr/pub/image-5K-9cuteberW2LPi1xWkgO3kdYF73F75eiWPggP1dcUQqcoe5K-PhtoedSjn8zTlSNzT/roy-levin.jpg)

![](https://tekkie.wordpress.com/wp-content/uploads/2014/06/michael-schroeder.gif?w=93&h=100)

From left to right: Andrew Birrell, from Microsoft Research; Roy Levin (not pictured), and Roger Needham, from ACM SIGSOFT

A couple graphical e-mail clients were developed by a team led by Doug Brotz, named â€œLaurelâ€ and â€œHardy,â€ which allowed easy review and filing of electronic mail messages. (sources: [The Xerox â€œStarâ€: A Retrospective](http://www.digibarn.com/friends/curbow/star/retrospect/)) FromÂ Lampsonâ€™s description, they appeared to beÂ â€œe-mail terminals.â€ They didnâ€™t store, or allow one to write e-mails. They just provided an organized display for them, and a means of telling a separate messaging system what you wanted to do with them, or that you wanted to create a new message to send. (Source: TAES) Andrew Birrell, Roy Levin, Roger Needham (who had a background in mathematics and philosophy), and Michael Schroeder, a computer scientist, developed a distributed service,Â which the e-mail clients used to compose, receive, and transmit network messages, called Grapevine. From the description, it appeared to work on a distributed peer-to-peer basis, with no central server controlling its services for an organization. Grapevine also provided authentication, file access control, and resource location services. Its purpose was to provide a way to transmit messages, provide network security, and find things like computers and printers on a network. It had its own name service by which clients could identify other systems. (source: [Grapevine: An Exercise in Distributed Computing](http://birrell.org/andrew/papers/Grapevine.pdf))Â To get a sense of the significance of this last point, the Arpanet did not have a domain name service, and the internet did not get its Domain Name Service (DNS) until the mid-1980s.

PARC had a complete office system going, with all of this, plus networked file systems, print servers (using Ethernet, created by Bob Metcalfe and Chuck Thacker at PARC), and laser printing (invented at PARC by Gary Starkweather, with software written by Butler Lampson) by 1975! A year later they had created a digital scanner.

You can see one of the e-mail clients being demonstrated, with WYSIWYG technology/laser printing, on the Alto, in this Xerox promotional video:

https://www.youtube.com/embed/M0zgj2p7Ww4?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en&autohide=2&wmode=transparent

![](https://tekkie.wordpress.com/wp-content/uploads/2014/06/clarence-ellis.jpg?w=135&h=135)

![](https://tekkie.wordpress.com/wp-content/uploads/2014/06/gary-nutt.jpg?w=90&h=128)

Clarence Ellis,
from the ACM

Gary Nutt,
from CU Boulder

Clarence Ellis and Gary Nutt, both computer scientists, developed OfficeTalk, a prototype office automation system, at PARC. It tracked â€œjobâ€ documents as they went from person to person in an organization. (source: â€œThe Xerox â€˜Starâ€™: A Retrospectiveâ€) In addition, Ellis came up with the idea of clicking on a graphical image to start up an application, or to issue a command to a computer, rather than typing out words to do the same thing. This is something we see in all modern user interfaces. (source: [Answers.com](http://www.answers.com/topic/clarence-a-ellis))

### Why Xerox missed the boat

For this section I go back to Waldropâ€™s book as my main source.

One might ask if the future in PCs was invented at Xeroxâ€“sounding an awful lot like what is in use with business IT systems todayâ€“why didnâ€™t they own the PC industry? An even bigger question, why werenâ€™t people back then using systems like what weâ€™re using now? We use Windows, Mac, and Linux systems today, each with their own graphical interfaces, which descended from what PARC created. We had word processing, and network LAN file servers for years, beginning in the 1980s, and later, e-mail over the internet, print servers, and later still, office â€œtaskâ€ automation software. The future of personal computing as we would come to know it was sitting right in front of them in the mid-1970s. We can say that now, since all of these ideas have been made into products we use in the work world. If we were to look at this technology back then, we mightâ€™ve been clueless to its significance. The executives at Xerox were an example of this.

For several years management couldnâ€™t understand the vision that was developing at PARC. The research culture there didnâ€™t mesh that well with the executive culture, especially after the companyâ€™s founder, Joe Wilson, died in 1971. Wilson had been open to new ideas and venturing into new technologies. At the time of his death, the company was also struggling from its own growth. The demand for its copiers was outstripping its ability to produce them competently. So a new management team was brought in which understood how to manage big companies. These were â€œnumbers men,â€ though, and their methodology didnâ€™t allow them to understand how to translate the kind of deep R&D the company had been doing with computers into products, because of course they didnâ€™t have metrics to make an accurate measurement of how much money theyâ€™d make with a technology that was totallyÂ unknown to them. The company had plenty of metrics about costs and revenue that came from copier development and sales, so it was no problem for them to make reliable estimates for a new copier model.Â The most profitable part of their business was copiers, so thatâ€™s where they put their product development resources. The old hands at Xerox who had set up PARC ran interference for their operation, to keep the â€œnumbers menâ€ from shutting down their work.Â The people at PARC kept trying to convince the higher-ups that what they had developed was useful for office productivity, but it was for naught.Â In a depressing anecdote, Waldrop wrote:

One top-level Xerox executive, after a day of being shown the wonders of PARC, had posed precisely one question to the researchers: â€œWhere can I get some of those beanbag chairs?â€ (TDM, p. 408)

(A famous â€œfeatureâ€ of PARC was their use ofÂ [beanbag chairs](http://www.computerhistory.org/revolution/input-output/14/348) for group discussions, called â€œdealer meetings.â€)

Stephen Lukasik came to Xerox from DARPA in 1975. He set up what was called the Systems Development Division (SDD). Its purpose was to create new salable products out of the research that was going on inside Xerox. The problem for him was that Xeroxâ€™s executives were willing to give him the authority to set up the division, but they werenâ€™t willing to listen to what he said was possible, nor were they willing to give him the funding to make it happen. Lukasik left Xerox in 1976. He said that though his time there was short, he valued the experience. He just didnâ€™t see the point in staying longer. Nevertheless, SDD would become useful to Xerox. It just had to wait for the right management.

Xerox set up a big demo of its products in Boca Raton, FL. in 1977 1976, which included the prototypes from PARC. All the company executives, their wives, family and friends were invited. The people from PARC did the best bang-up job they could to make their stuff look impressive for business computing.

Back at PARC, says [Gary] Starkweather, he and his colleagues saw this as their last, best chance. â€œThe feeling was, â€˜If they donâ€™t get this, we donâ€™t know what we can do.'â€ So, he says, with John Ellenby coordinating an all-out effort, â€œWe stripped everything out of PARC down to the power cords and set it all up again in Boca. Computers, networks, printersâ€“the whole thing! I built a laser printer that did color. Bill English had a word processor that did Japanese. We were going to show them space flight!â€

They certainly tried. [At the event], Xerox executives and their families swarmed through the Grenada Rooms of the Boca Raton Hotel for a hands-on demonstration of WYSIWYG editing in Bravo, graphical programming in Smalltalk, e-mailing in Laurel, artistry in Paint and Drawâ€“the works. â€œThe idea was a mental slam-dunk!â€ says Starkweather. â€œAnd some people did see it.â€ The executivesâ€™ wives, for exampleâ€“many of them former secretaries who knew all about carbon paper, Wite-Out, and having to retype whole pages to correct a single mistakeâ€“took one look at Bravo and *got* it. â€œThe wives were so ecstatic they came over and kissed me,â€ remembers Jack Goldman. â€œThey said, â€˜Wonderful things youâ€™re doing!â€™ Years later, Iâ€™d see them and theyâ€™d still remember Boca.â€

Then there were the delegates from Fuji Xerox, the companyâ€™s Japanese partner: they were beside themselves over Bill Englishâ€™s word processor. â€œFuji clamored, â€˜Give us this! Weâ€™ll manufacture it!'â€ recalls Goldman. In fact, he says, that was a near-universal reaction: â€œPeople from Europe, people from South America, marketing groups around the U.S.â€“everyone who went out of that conference was excited by what they had seen.â€

Everyone, that is, except the copier executives, the real power brokers in Xerox. You couldnâ€™t miss them; they were the ones standing in the background with the puzzled, So-What? look on their faces.

â€¦

Indeed, one of the corporationâ€™s purposes in calling the conference was to rally the troops for the coming era of ever-more-ferocious competition. In fairness, those executives in the background had to worry about defending the homelandÂ *now*, not ten years from now. Or maybe they were simply too bound by the culture of the executive suite, vintage 1977. The xerographers lived in a world in which typing was womenâ€™s work and keyboards were for secretaries. It was a rare executive who would even deign to touch one. (TDM, p. 408)

There was a change in leadership in 1978 which recognized that the management style they had been using wasnâ€™t working. The Japanese had entered the copier market, and were taking market share away from them. This is just what Joe Wilson had anticipated eight years earlier. Secondly, the new management recognized the vision at PARC. They wanted to create products from it. Work on the Xerox Star system began that year at SDD.

SDD had previously created a machine called Dandelion, Xeroxâ€™s most powerful computer to date. The Star would be the Dandelion translated into a business system. (Source: TAES)

### Xerox and Apple

Thereâ€™s been increasing awareness about the history of Xerox and Apple as time has passed, but there are some misconceptions about it that deserve to be cleared up.

Xerox and Apple developed a brief business relationship in 1979. Apple got ideas about how to create the user experience with their Lisa and Macintosh computers from Xerox PARC. The misperceptions are inÂ *how*Â this happened.Â This is how the meeting between Xerox and Apple in 1979 has been perceived among those who are generally familiar with this history (from the 1999 TNT made-for-TV movieÂ *â€œPirates of Silicon Valleyâ€*):

https://www.youtube.com/embed/2u70CgBr-OI?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en&autohide=2&wmode=transparent

Thereâ€™s some truth to this, but some of it is myth. One could almost say it was trumped up to bolster Steve Jobsâ€™s image as a visionary. Itâ€™s a story thatâ€™s been propagated for many years, including by yours truly. So, I mean to correct the record.

Much of Waldropâ€™s account of what happened between Xerox and Apple is a retelling of an account from Michael Hiltzikâ€™s book,Â *â€œDealers of Lightning.â€*Â From what he says, the Apple teamâ€™s visit at PARC wasÂ *not*Â a total revelation about graphical user interfaces, but it was an inspiration for them to improve on what they had developed.

The idea of a graphical user interface on a computer was already out there. People at Apple knew of it prior to visiting Xerox. Xerox had been publishing information about the concept, and had been holding public demos of some of the graphical technology PARC had developed. So the idea of a graphical user interface was not a secret at all, as has been portrayed. However, this does not mean that every aspect of Xeroxâ€™s user interface development was made public, as Iâ€™ll discuss below.

Apple had been working on a computer called [Lisa](http://en.wikipedia.org/wiki/Apple_Lisa) since 1978. It was a 16-bit machine that had a high-resolution bitmapped display. The Lisa team called it a â€œgraphical computer.â€ Apple was approached by the Xerox Development Corporation (XDC) to take a look at what PARC had developed. The director of XDC felt that the technology needed to be licensed to start-ups, or else it would languish. Steve Jobs rebuffed the offer at first, but was convinced by Jef Raskin at Apple to formÂ a team to go to PARC for a demonstration.

Apple and Xerox made a trade. Xerox bought a stake in Apple that was worth about $1 million,Â in exchange for Apple getting access to PARCâ€™s technology. Waldrop says that Jobs and a team of Apple engineers made two visits to PARC in December 1979. According to an [article](http://www-sul.stanford.edu/mac/parc.html) fromÂ Stanford University, Jobs was not with them for the first visit. Waldrop notes that by this point Apple had already added a graphical user interface to the Lisa, but that it was clunky. By Larry Teslerâ€™s account, there were more visits by the Apple team than Waldrop talks about. He has a different recollection of some of the details of the story as well. I include these different sources to give a â€œspectrumâ€ of this history.

https://www.youtube.com/embed/ZOF-j6Nxm04?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en&autohide=2&wmode=transparent

Going by Waldropâ€™s account, at their first visit in December â€™79, they got a â€œstandardâ€ demo of the Alto, given by Adele Goldberg, that many other visitors to PARC had already seen. The group from Apple saw it being used with a mouse, the Bravo word processor, some drawing programs, etc., and then they left, apparently satisfied with what they had seen. It should be noted that this demo did not show the Xerox â€œdesktop interfaceâ€ in the sense of what people have come to know on personal computers and laptops. Each program they saw was a separate entity, which took over the entire operation of the machine, using the Altoâ€™s graphical capabilities to show graphics and text. The Apple team did not see the desktop metaphor, which was in Smalltalk, and which was being developed for the Xerox Star. Goldberg considered Smalltalk proprietary. Jobs and the team eventually realized that they hadnâ€™t really seen â€œthe good stuff.â€

The Apple team came back to PARC, unannounced. This is the visit thatâ€™s usually talked about in Apple-PARC lore, and is portrayed in the *Pirates of Silicon Valley*Â clip above. JobsÂ *demanded*Â to see the Smalltalk systemÂ ***NOW***. A heated argument ensued between PARC and Xerox headquarters over this. Xeroxâ€™s executives ordered the PARC team to demonstrate Smalltalk to the Apple team, citing the partnership with Apple brokered by XDC. Bob Taylor was out of town at the time. He later said that he had no respect for Steve Jobs, and if he had been there, he wouldâ€™ve kicked the Apple team out of the building, no ifâ€™s, andâ€™s, or butâ€™s! He figured Xerox wouldâ€™ve fired him for it, but that wouldâ€™ve been fine with him.

This time the people from Apple were prepared. They asked very detailed technical questions, and they got to see what Smalltalk was capable of. They saw educational software written by Goldberg, programming tools written by Larry Tesler, and animation software written by Diana Merry that combined graphics with text in a single document. They got to see multiple tasks on the screen at the same time with overlapping windows, in a â€œdesktopâ€ metaphor. Jobs was beside himself. He exploded, â€œWhy hasnâ€™t this company brought this to market?! Whatâ€™s going on here? I donâ€™t get it!â€Â This was when Jobs had his epiphany about the graphical user interface, though Waldrop doesnâ€™t delve into what that was. The breakthrough for him may have been the desktop metaphor, how it allowed multiple, different views of information, and multiple tasks to be worked on at the same time, along with everything else heâ€™d seen. The way Waldrop portrays it is that Jobs realized that using a computer should be a fulfilling and fun experience for the person using it.

According to Hiltzik, the partnership between Xerox and Apple, of which the Apple stock trade had been a part, quickly fell apart after this, due to a culture clash. The Lisaâ€™s chief programmer, Bill Atkinson, had to go off of what he remembered seeing at PARC, since he no longer had access to their detailed technical information.

So the idea that Apple (and Microsoft for that matter) â€œstoleâ€ stuff from Xerox is not accurate, though there was trepidation at PARC that Apple would steal â€œthe kitchen sink.â€ While the visit gave the Lisa team ideas about how to make computer interaction better, and what the potential of the GUI was, Hiltzik said it wasnâ€™t that big of an influence on the Lisa, or the Macintosh, in terms of their overall system design.

What this account means is that the visits to PARC were tangentially influential on Appleâ€™s version of the idea of a graphical user interface. They did not go in ignorant of what the concept was, and it did not give them all of the ideas they needed to create one. It just helped make their idea better.

### The microcomputer â€œrevolutionâ€

The people at PARC were aware of the nascent microcomputer phenomenon that was occurring under their noses. Some of them went to the Homebrew Computer Club meetings, where Steve Jobs and Steve Wozniak used to hang out.Â They read the upstart computer press that was raving about what Bill Gates and Steve Jobs were doing with their new companies, Microsoft and Apple Computer. According to Waldrop, it galled them that this upstart industry was getting so much attention and adoration that they felt it didnâ€™t deserve.

â€œIt had never occurred to us that people would buy crap,â€ declares Alan Kay, who considered the hobbyists in their garages down the hill to be very bright and very creative ignoramusesâ€“undisciplined kids who didnâ€™t read and didnâ€™t have a clue about what had already been done. They were successful only because their customers were just as unsophisticated. â€œWhat none of us was thinking was that there would be millions of people out there who would be perfectly happy with the McDonaldâ€™s hamburger approach.â€ (TDM,Â p. 437)

Steve Jobs was somewhat of an exception. At least he got a hint of â€œwhat had already been done.â€ It took some prodding, but once he got it, he paid attention. Still, he didnâ€™t fully understandÂ the significance of the research that went into what he saw. For example, Jobs was very hostile to the idea of computer networking at the time, because he thought that would deprive the personal computer user of their autonomy. Freedom from dependency on larger computer systems was a notion he held to ideologically. When he railed against IBM as â€œbig brotherâ€ it wasnâ€™t just for show. If only he had been aware of Lickliderâ€™s vision for the internet (which was published at the time), the notion of a distributed network, with independent units, and the DARPA work that was creating it, perhaps he wouldâ€™ve cottoned to it the way he had the graphical user interface. Weâ€™ll never know. He came to understand the importance of the network features that had been developed at PARC about 6 years later when he left Apple and started up NeXT.

Thereâ€™s a famous quote from Jobs about his visits to PARC that illustrates what Iâ€™m talking about, from the PBS mini-series,Â *â€œTriumph of the Nerdsâ€* (I wrote a post about this mini-series [here](https://tekkie.wordpress.com/2007/04/02/triumph-of-the-nerds/)):

They showed me, really,Â three things, but I was so blinded by the firstÂ one that I didnâ€™t reallyÂ â€seeâ€Â the other two. One of the things they showed me was object-oriented programming. They showed me that, but I didnâ€™t even â€œseeâ€ that. The other one they showed me was really a networked computer system. They had over 100 Alto computers all networked, using e-mail, etc., etc. I didnâ€™t even â€œseeâ€ that. I was so blinded by the first thing they showed me, which was the graphical user interface. I thought it was the best thing I had ever seen in my life. Now, remember it was very flawed. What we saw was incomplete. They had done a bunch of things wrong, but we didnâ€™t know that at the time.Â Still, though, the germ of the idea was there, and they had done it very well. And withinÂ ten minutes it was obvious to me that all computers would work like this, someday.

### Too much, too late for Xerox

Xerox released the Star in 1981 as the 8010 Information System.

https://www.youtube.com/embed/mhrC8WXESJo?feature=oembed

It had a Smalltalk-like graphical user interface, anywhere from 384 kilobytes of memory up to 1.5 MB, an 8â€³ floppy drive, a 10 to 40 MB hard disk, a monitor measuring 17â€³ diagonally, a mouse, a bevy of system features, Ethernet, and laser printing. (source: [The Xerox â€œStarâ€: A Retrospective](http://www.digibarn.com/friends/curbow/star/retrospect/)) The 8010 cost $16,500 per unit, though a customer couldnâ€™t purchase just the computer. It was designed to be an integrated system, with networking and laser printing included. A minimal installation cost $100,000 (about $252,000 in todayâ€™s money). Xerox was going for the Fortune 500, which could afford expensive, large-scale systems. Even though the designers thought of it as a â€œpersonal computerâ€ system, the 8010 was marketed under the old mainframe business model.

The designers at SDD put the kitchen sink into it.Â It had every cool thing they could think of. The system was designed so thatÂ *no* third-party software could be installed on itÂ *at all*. All of the hardware and software came from Xerox, and had to be installed by Xerox employees. This was also par for the course with typical mainframe setups.

In the Star operating system, all of the software was loaded into memory at boot-up, and kept memory-resident (very much like Smalltalk). Users didnâ€™t worry about what applications to run. All they had to focus on was their documents, since all documents and data were implicitly linked to the appropriate software. It presented an object-oriented approach to information. It didnâ€™t say to you, â€œYouâ€™re working with a word processor.â€ Instead, you worked with your document. The system software just accommodated the document by surreptitiously activating the appropriate part of the system for its manipulation, and presenting the appropriate interface.

Since the Xerox brass recognized they knew nothing about computers, they turned the Starâ€™s design totally over to SDD. There appeared to have been no input from marketing. Even so, as Steve Jobs said of the executives, â€œThey were copier-heads. They just had no clue about a computer, what it could do.â€ Itâ€™s unclear whether input from marketing wouldâ€™ve helped with product development. I have a feeling â€œthe innovatorâ€™s dilemmaâ€ applies here.

The designers at SDD were also told that the cost of the product was no object, since their target market was used to paying hundreds of thousands of dollars for large-scale systems. The Xerox executives miscalculated on this point. While itâ€™s true that their target market had no problem paying the systemâ€™s price tag, personal computing was a new, untested concept to them, and they were wary about risking that much money on it. Xerox didnâ€™t have a low-risk, low-cost entry configuration to offer. It was all or nothing. Microcomputers were a much easier sell in this environment, because their individual cost, by comparison, went unnoticed in corporate budgets. Corporate managers were able to sneak them in, buying them with petty cash, even though IT managers (who believed wholeheartedly in mainframes) tried their darnedest to keep them out.

The microcomputer market, which just about everyone at Xerox saw as a joke, was eating their lunch. Bob Frankston and Dan Bricklin at VisiCorp (both alumni of the Project MAC/Multics project) had come out with VisiCalc, the worldâ€™s first commercial spreadsheet software, in 1979, and it was only available for micros. Xerox didnâ€™t have an equivalent on the 8010 when it was released. They had put all of their â€œeggsâ€ into word processing, databases, and e-mail. These things were needed in business, but it was the same thing the researchers at PARC had run into when they tried to show the Alto to the Xerox brass in years passed: People in the corporate hierarchy saw themselves as having certain roles, and they thought they needed different tools than what Xerox was offering. Word processing was what stenographers needed, in the minds of business customers. Managers didnâ€™t want it. They wanted spreadsheets, which were the â€œkiller applicationsâ€ of the era. They would buy a microcomputer just so they could run a spreadsheet on it.

The designers at SDD didnâ€™t understand their target market.Â The philosophy at PARC was â€œeat your own dog foodâ€ (though I imagine they used a different term for it). From what Iâ€™ve heard, listening to people who were part of the IPTO in the 1960s, it was Doug Engelbart who invented this development concept. The problem for Xerox was this applied to product development as well as to overall quality control. From the beginning, they wanted to use all of the technology they developed, internally, with the idea that if they found any deficiencies, there would be no running away from them. It would motivate them to make their systems great.

Thacker and Lampson mentioned in a retrospective, which I refer to at the end of this post, that someone at PARC had come up with a spreadsheet for the Alto, but that nobody there wanted to use it. They were not accountants. Xerox eventually released a spreadsheet for the 8010, once they saw the demand for it, but the writing was already on the wall by then.

Itâ€™s hard for me to say whether this was intentional, but the net effect of the way SDD designed the Star resulted in an implicit assumption that customers in their target market would want the capabilities that the designersÂ thought were important. Their focus was on building a complete integrated system, the likes of which no one had ever seen. The problem was nothing in their strategy accounted for the needs and perceptions that business customers would assert. They may have assumed that customers would be so impressed by the innovativeness of the system that it would sell itself, and people would adjust their roles to it in a McLuhan-esque â€œthe medium is the messageâ€ sort of way.

The researchers at PARC recognized that producing a microcomputer had always been an option. In 1979, Bob Belleville suggested going with a 16-bit microcomputer design, maybe using the Motorola 68000, or the Intel 8086 processor, instead of going forward with the Star. He built a prototype and showed that it could work, but the team working on the Star didnâ€™t have the patience for it. They wouldâ€™ve had to throw out everything they had done, hardware and software, and start over. Secondly, they wouldnâ€™t have been able to do as much with it as they were able to do with the approach they had been pursuing. Thacker and Lampson saw as well that building a microcomputer would be more expensive than going ahead with their approach. It was, however, a fateful decision, because the opposite would be true two years later, when the 8010 Information System was released.

Lampson said, ironically, that this time, â€œthe problem wasnâ€™t a shortage of vision at headquarters. If anything, it was an excess of vision at PARC.â€

WhenÂ Apple released their Lisa computer in 1983, Xerox realized that they had missed their chance. The future belonged to IBM, Apple, and Microsoft.

### Dispersion

From about 1980 on, people left PARC to join other companies. They were bleeding talent. Taylor was seen as part of the problem. He had an overriding vision, and it was his way, or the highway, so others have said. He understood full well where computing was going. He was just ahead of his time. He could be very supportive, if you agreed with his vision, but he had utter contempt for any ideas he didnâ€™t approve of, and he would â€œtake out the flamethrowerâ€ if you opposed him. This sounds a lot like what people once complained about with Steve Jobs, come to think of it. The director of PARC kindly asked Taylor to change his attitude. He took it as an insult, andÂ left in frustration in 1983, taking some of PARCâ€™s best engineers with him.

From looking at the story, it appears the failure of the Star project was â€œthe last strawâ€ for Xeroxâ€™s foray into computer research. After Taylor left, the era of innovative computer research ended at PARC.

The visions at Xerox were grandiose, and were too much, too late for the market. I donâ€™t mean to take anything away from what they did by saying this. What they had was pretty good. It represented the future, but it was too far ahead of where the business computing market was. Competitors had grabbed the attention of customers, and they preferred what the competition was offering.

The PARC researchers got it both ways. When they had the chance to develop products in 1975, ahead of the explosion in the microcomputer market, their vision wasnâ€™t recognized by the company that housed them. By the time it was recognized, the market had changed such that much less powerful machines, backed by more innovative business models, won out.

Even though Alan Kay brought the idea of a small, handheld computer to PARC, something that ordinary people would love to use, he valued computing power over the size of the machine. As he would later say about that period, the idea of the Dynabook was more of a service metaphor. The size and form of the hardware was incidental to the concept. (source: [An Interview with Computing Pioneer Alan Kay](http://techland.time.com/2013/04/02/an-interview-with-computing-pioneer-alan-kay/)) Thacker and Lampson, the designers of the 8010, shared that value as well. By the late 1970s the market was thinking â€œsmall is beautiful,â€ and they were willing to tolerate clunky, low-powered machines, because their economies of scale met immediate needs, and they created less friction from corporate politics.

Apple and Microsoft used ideas developed at PARC to further develop the personal computer market, and so watered down pieces of that vision got out to customers over about 20 years. Today we live in a world that resembles what Bob Taylor envisioned, with individual computers, networking, and digital printing, using graphical systems.

### PARCâ€™s legacy

The outside world would come to know the desktop graphical user interface metaphor because of Smalltalk, though the metaphor was repurposed away from Alan Kayâ€™s powerful ideas about a modeling system, into a system that made it easy to run applications, and emulated integrating older media together into virtual paper documents.

To me, the most interesting contributor to the desktop interface was Diana Merry. She was originally hired at PARC as a secretary. She just happened to understand Alan Kayâ€™s goals, and so she was invited into the LRG. She and Kay created the first implementation of overlapping windows, in Smalltalk. She also wrote a lot of the basic software Smalltalk used to display and animate graphics. (Sources: *â€œThe Mouse and the Desktop â€” Designing Interactions,â€* by Bill Moggridge, and TEHS)

The Paint, Draw, and Write applications that appeared on the Apple Lisa and the first Macintosh systems were inspired by similar works that had been created years earlier at PARC.

Microsoft Windows, and Microsoft Word benefitted from the research that was done at PARC, though some inspiration came from the Macintosh as well. The look and feel of the first versions of Windows owed more to the [X/Window system](http://en.wikipedia.org/wiki/X_Window_System) on Unix. Where Microsoft copied Apple was in how people interacted with Windows (icons and menus), and the application suite that came with the system.Â (source:Â [The Secret Origin of Windows](http://technologizer.com/2010/03/08/the-secret-origin-of-windows/))

Software developers who have been working on apps. for Apple products in the most recent generations of systems may be interested to know that the Objective-C language theyâ€™ve used was first created by a company called StepStone in the early 1980s. The design of the language and its runtime were somewhat influenced by the Smalltalk system.

Several companies licensed a version of Smalltalk, called Smalltalk-80, from Xerox during the 1980s. Among them were Tektronix, Hewlett-Packard, Digital Equipment Corp., and Apple. (source: [â€œSmalltalk-80: Bits of History, Words of Adviceâ€](http://stephane.ducasse.free.fr/FreeBooks/BitsOfHistory/BitsOfHistory.pdf)) Apple got a version running on the Macintosh XL in 1985. (source: [The Long View](http://basalgangster.macgui.com/RetroMacComputing/The_Long_View/Entries/2010/3/6_Smalltalk-80.html)) Appleâ€™s version of Smalltalk was updated in the mid-1990s by Alan Kay and some of the original Learning Research Group gang from Xerox. They got Appleâ€™s permission to release it into the public domain, and it has since been ported to many platforms. Professional developers call it by its new name, [â€œSqueak.â€](http://squeak.org/) An educational version also exists, maintained by the Squeakland Foundation, which goes by the name [â€œEtoys.â€](http://squeakland.org/)

Charles Simonyi joined Microsoft. He brought his knowledge from developing Bravo with him, and it went into creating Microsoft Word. He became the lead developer on their Multiplan, and Excel spreadsheet software.

### Where are they now?

(**Edit** 4/15/2017:Â As events unfold, I will be updating this section.)

[Stephen Lukasik](http://www.iac.gatech.edu/faculty-and-staff/faculty/bio/lukasik)Â became a Chief Scientist at the FCC from 1979-1982. He is a member of theÂ International Institute for Strategic Studies, the American Physical Society, and the American Association for the Advancement of Science. He is a founder of The Information Society journal, and has served on the Boards of Trustees of Harvey Mudd College and Stevens Institute of Technology. (Source: Georgia Tech)

[Larry Roberts](http://www.packet.cc/) was chief executive at Telenet until 1980. Telenet was sold to GTE in 1979 and subsequently became the data division of Sprint. In 1983 he became the CEO of NetExpress, an Asynchronous Transfer Mode (ATM) equipment company. Roberts then became president of ATM Systems from 1993 to 1998. He then went back to packet networking, founding Caspian Networks, which focused on IP flow management (IP as in â€œInternet Protocolâ€), until 2004. He founded Anagran in an effort to do what Caspian did, but more efficiently. (Source: Larry Robertsâ€™s home page) He died on December 26, 2018. (Source: [Early internet pioneer Larry Roberts dies at 81](https://www.engadget.com/2018/12/30/arpanet-creator-larry-roberts-dies/) â€” Engadget)

J.C.R. Licklider â€“ In the late 1970s, Lick visited Xerox PARC regularly. He served on the Committee on Government Relations at the Association of Computing Machinery (ACM), and as deputy chairman of the Social Security Administrationâ€™s Data Management System. He spent a year in 1978 on a task force commissioned by the Carter Administration, which examined the governmentâ€™s data-processing needs. He served as president of the Boston Computer Society. He was an investor and advisor to a company called Infocom in 1979, which was founded by eight of his former students from his Dynamic Modeling group at MIT. He spent part of his time working at [VisiCorp](http://en.wikipedia.org/wiki/VisiCorp). HeÂ continued to work at MIT until he retired in 1985.Â He died on June 26, 1990.

[Ed Feigenbaum](http://en.wikipedia.org/wiki/Ed_Feigenbaum) â€“ In 1984, he became a Fellow at the American College of Medical Informatics. From 1994 to 1997 he served as Chief Scientist of the U. S. Air Force. He founded the Knowledge Systems Laboratory at Stanford University, and is now professor emeritus at Stanford. He was co-founder of several start-ups, such as IntelliCorp, Teknowledge, and Design Power, Inc. He has served on the National Science Foundation Computer Science Advisory Board, on the National Research Councilâ€™s Computer Science and Technology Board, and as a member of the Board of Regents of the National Library of Medicine. He is a Fellow at the the Association for the Advancement of Artificial Intelligence, the American Institute of Medical and Biological Engineering, and of the American Association for the Advancement of Science. He is a member of the National Academy of Engineering and of the American Academy of Arts and Sciences. (Source:Â [Feigenbaumâ€™s Turing Award citation at the ACM](http://amturing.acm.org/award_winners/feigenbaum_4167235.cfm))

[George Heilmeier](http://en.wikipedia.org/wiki/George_H._Heilmeier)Â became vice president at Texas Instruments in 1977. In 1983 he was promoted to Senior Vice President and Chief Technical Officer. In his current position, he is responsible for all TI research, development, and engineering activities.Â From 1991-1996 he also served as president and CEO of Bellcore (now Telcordia), ultimately overseeing its sale to Science Applications International Corporation (SAIC). He served as the companyâ€™s chairman and CEO from 1996-1997, and afterwards as its chairman emeritus.Â He serves on the board of trustees of Fidelity Investments and of Teletech Holdings, and the Board of Overseers of the School of Engineering and Applied Science of the University of Pennsylvania. HeÂ is a member of the National Academy of Engineering, the Defense Science Board, and is Chairman of the Technical Advisory Board of Southern Methodist University.Â (sources: Wikipedia, andÂ [IEEE Global History Network](http://www.ieeeghn.org/wiki/index.php/George_H._Heilmeier))

It should be noted that Heilmeier is credited with being the inventor of the Liquid Crystal Display (LCD), a technology Alan Kay hoped to use for his Dynabook in the 1970s, and which became the basis for the digital display most of us use today.

[Alan Kay](http://en.wikipedia.org/wiki/Alan_Kay)Â left PARC on sabbatical in 1980, and never came back. He came to Atari in 1981, as Chief Scientist, doing research on interactive computing (you can see some samples of it [here](https://youtu.be/KMKVTbcF2ro)). He then joined Apple as a research Fellow in 1984, where he worked on improving education in conjunction with technology. He joined Disney as a Fellow and Imagineer from 1996 to 2001. He founded Viewpoints Research Institute (VPRI) in 2001. He then became a research fellow at Hewlett-Packard from 2002 to 2005. During that time he was a Visiting Professor at Kyoto University, an adjunct professor of Computer Science at MIT, and was involved with the development of the [XO Laptop](http://laptop.org/en/laptop/), developed by the One Laptop Per Child program at MIT. Today he is an adjunct professor of Computer Science at UCLA, and he continues his work at Viewpoints. He is also on the advisory board of TTI/Vanguard. From 2006 to about 2012, Viewpoints worked on a project, sponsored by the National Science Foundation, to reinvent personal computing. (sources: [The New York Times](http://www.nytimes.com/1984/05/03/business/key-atari-scientist-switches-to-apple.html),Â [Chap. 12](http://www.rheingold.com/texts/tft/12.html)Â from Howard Rheingoldâ€™s â€œTools For Thought,â€ Wikipedia,Â [Bio. on Alan Kay at Answers.com](http://www.answers.com/topic/alan-kay),Â [VPRI: Inventing Fundamental New Computing Technologies](http://www.vpri.org/html/work/ifnct.htm)). In 2016, Kay joined with Y-Combinator to form the Human Advancement Research Community, or HARC (see: [HARC](https://harc.ycr.org/)). Work at VPRI ended in 2018.

[Bob Taylor](http://en.wikipedia.org/wiki/Robert_Taylor_(computer_scientist))Â went on to found the Systems Research Center (SRC) at Digital Equipment Corp. (DEC) in 1984. He retired from DEC in 1996. He died on April 13, 2017.

[Butler Lampson](http://en.wikipedia.org/wiki/Butler_Lampson)Â also left PARC with Taylor, and joined him at Digital Equipment Corp. He became a Fellow of the ACM in 1992. He now works at Microsoft Research, and is an adjunct professor at MIT. He became a Fellow at the Computer History Museum in 2008.

[Dan Ingalls](https://www.linkedin.com/pub/daniel-ingalls/b/b34/45b)Â left PARC to work at Apple in 1984. Beginning in 1987 he helped run the Homestead Hotel, a family business, until 1993. He stayed on with Apple until 1996. From 1996 to 2001 he was Principal Staff Director at Disney Imagineering. He worked as a consultant for Hewlett-Packard and at Viewpoints Research Institute from 2001 to 2005. He joined Sun Labs in 2005, where he developed his newest project, called Lively Kernel. He left Sun in 2010 to become a Fellow at SAP, where he continues to work today. He continues development of his Lively Kernel Project at the Hasso Plattner Institute. (Sources: Dan Ingallsâ€™s Linkedin page, and his [Wikipedia page](http://en.wikipedia.org/wiki/Dan_Ingalls))

Diana Merry â€“ After leaving Xerox in 1986, she has continued her work in Smalltalk with various employers. You can see her complete work history at her [LinkedIn page](http://www.linkedin.com/in/dianamerryshapiro).

[Chuck Thacker](http://research.microsoft.com/en-us/people/cthacker/)Â left PARC the same time Bob Taylor did, and joined DEC as a founder of its Systems Research Center. He then joined Microsoft in 1997 to help found Microsoft Research in Cambridge, UK. He returned to the U.S., and developed the technology which was used in Microsoftâ€™s Tablet PC. He continued working at Microsoft Research in Silicon Valley on computer architecture for many years. He died on June 12, 2017. (source: [Arstechnica](https://arstechnica.com/business/2017/06/charles-thacker-key-designer-of-the-xerox-alto-dies-at-74/))

[Adele Goldberg](http://en.wikipedia.org/wiki/Adele_Goldberg_(computer_scientist)) became president of the Association of Computing Machinery (ACM) from 1984 to 1986, and continued to have a long association with the organization, taking on various roles. She continued on at PARC until 1987. She wanted to make sure that Smalltalk technology got out to a wider audience, so she worked out a technology exchange agreement with Xerox in the late 1980s and founded ParcPlace Systems, which commercialized a version of Smalltalk. She served as CEO and chairwoman of ParcPlace until 1995, when the company merged with Digitalk, another Smalltalk vendor, to become ParcPlace-Digitalk. In 1997 the company changed its name to ObjectShare. In 1999 ObjectShare was sold to Cincom, which has continued to develop and sell a Smalltalk development suite. Goldberg was inducted as a Fellow at the ACM in 1994. She co-founded [Neometron](http://www.neometron.com/main/home/body.html)Â in 1999,Â and now also works atÂ [Bullitics](http://visualmerc.net/clients/bullitics/). She is a board member of Cognito Learning Media. She has continued her interest in education, formulating computer science courses at community colleges in the U.S. and abroad.

[Charles Simonyi](http://en.wikipedia.org/wiki/Charles_Simonyi)Â went to work at Microsoft in 1981. He led their development efforts to create Word, Multiplan, and Excel. He left Microsoft in 2002 to found [Intentional Software](http://www.intentsoft.com/), where heâ€™s been at work on what Iâ€™d call his â€œDomain-Oriented Developmentâ€ software and techniques.

[Larry Tesler](http://en.wikipedia.org/wiki/Larry_Tesler) went to work at Apple in 1980, becoming Vice President of the Advanced Technology Group, and Chief Scientist. He worked on the team that developed the Lisa computer. In 1990 he led the effort to develop the Apple Newton, one of the first of what weâ€™d recognize as a personal digital assistant. Tesler left Apple in 1997 to co-found a company called Stagecast Software. In 2001 he joined Amazon.com as its Vice President of Shopping Experience. In 2005 he joined Yahoo! as Vice President of its User Experience and Design group. In 2008 he went to work for 23andMe, a personal genetics information company. He was a consultant, starting in 2009. He died on February 16, 2020.

Timothy Mott left Xerox PARC to co-found Electronic Arts in 1982. In 1990 he became Director at Electronic Arts, and co-founded Macromedia, staying with them until 1994. In 1995 he co-founded Audible.com, and stayed with them until 1998. He stayed on with Electronic Arts until 2007. You can see his complete work profiles at [Bloomberg BusinessWeek](http://investing.businessweek.com/research/stocks/private/person.asp?personId=564669&privcapId=120226&previousCapId=23267&previousTitle=Technology%20Crossover%20Ventures) andÂ [CrunchBase](http://www.crunchbase.com/person/timothy-mott).

Doug Brotz â€“ Itâ€™s been difficult finding much information on him. All I have is that he joined Adobe and was a [co-developer of the PostScript laser printer control language](http://www.adobe.com/products/postscript/pdfs/postscript_is_20.pdf).

[Andrew Birrell](https://birrell.org/andrew/me/bio.php)Â â€“ He left Xerox PARC in 1984 to joinÂ the Systems Research Center at DEC, where he worked on Network Objects, a distributed object system, Virtual Paper, a system for easy online document reading, and the Personal Jukebox, the first multi-gigabyte portable audio player. DEC was acquired by Compaq in 1998. He stayed with Compaq until 2001, when he went to Microsoft Research. He retired in 2014. He died on December 7, 2016.Â (Source:Â [the ACM](https://cacm.acm.org/news/210689-in-memoriam-andrew-birrell-1951-2016/fulltext))

[Roy Levin](http://research.microsoft.com/en-us/people/roylevin/) became a senior researcher at the DEC Systems Research Center in 1984. In 1996, he became its director. In 2001 he co-founded Microsoft Research in Silicon Valley, became a Distinguished Engineer, and its Managing Director. He continues work there today.

[Roger Needham](http://www.cl.cam.ac.uk/archive/ksj21/RogerNeedhamBriefBiog.pdf) worked as a consultant at Xerox PARC from 1977-1984. He then worked at DECâ€™s Systems Research Center from 1984-1997, and at Hitachi Advanced Research Laboratory from 1994-1997. He became a Fellow at the Royal Academy of Engineering in 1993, and of the ACM in 1994. He was a Fellow at Wolfson College, in Cambridge, UK, from 1966-2002.Â He joined Microsoft Research in 1997, and founded their European Research Labs. He was a longtime member of the International Association for Cryptographic Research, the IEEE Computer Society Technical Committee on Security and Privacy, and the University Grants Committee, an advisory committee to the British government. He died on March 1, 2003. (source: [Wikipedia](http://en.wikipedia.org/wiki/Roger_Needham))

[Michael Schroeder](http://en.wikipedia.org/wiki/Michael_Schroeder) â€“ I havenâ€™t found detailed information on Schroeder, except to say that as a professor at MIT he worked on the Multics project (I covered this project in [Part 2](https://tekkie.wordpress.com/2012/02/04/a-history-lesson-on-government-rd-part-2/) of this series), before coming to Xerox PARC, and that after PARC, he worked at DECâ€™s Systems Research Center. He then came to Microsoft Research in 2001, where he continues to work today. He became a Fellow of the ACM in 2004.

[Clarence Ellis](http://www.answers.com/topic/clarence-a-ellis) â€“ After leaving Xerox PARC, he became head of the Groupware Research Program at the Microelectronics Computer Consortium in Austin, TX. He also worked at Los Alamos Labs, and Argonne National Laboratory. He held academic positions at Stanford, the University of Texas, MIT, and the Stevens Institute of Technology. In 1991, he became the chief architect of the FlowPath workflow product at Bull S.A. In 1992, he came to the University of Colorado at Boulder as a professor of computer science, and, with Gary Nutt, formed the Collaborative Technology Research Group (CTRG) to work on project workflow systems. He became professor emeritus of computer science at CU in 2010. He was on the editorial board of various journals. He was a member of the National Science Foundation (NSF) Computer Science Advisory Board, the University of Singapore ISS International Advisory Board, and the NSF Computer Science Education Committee. He died on May 17, 2014. (sources: [Computer Scientists of the African Diaspora](http://www.math.buffalo.edu/mad/computer-science/ellis_clarencea.html),Â [CTRG Groupware and Workflow Research](http://www.cs.colorado.edu/~skip/workshop/ellis.html), [the Daily Camera](http://www.dailycamera.com/cu-news/ci_25824569/cu-boulder-professor-clarence-skip-ellis-remembered-warm))

A note I found in Ellisâ€™s bio. also says that he was the first African-American to receive a Ph.D. in computer science, in 1969, from the University of Illinois at Urbana-Champaign. He did his post-graduate work developing the [Illiac IV supercomputer](http://en.wikipedia.org/wiki/ILLIAC_IV), an ARPA/IPTO project, and one of the first massively parallel computer systems.

[Gary Nutt](http://www.gnutt.com/GNutt/index.html) worked at Xerox PARC from 1978-1980. He worked on collaboration systems at Bell Labs in Denver, CO. from 1980-81. He took a couple executive roles at technology firms in Boulder, CO. from 1981-1986. He returned to the University of Colorado at Boulder in 1986 as a CS professor (he was previously an associate CS professor at CU Boulder from 1972-1978). While on sabbatical in 1993, he worked for Group Bull in Paris, France, developing collaboration products. In 2000, he worked as VP of Engineering for Bookface.com, managing their intellectual property. He went to Inktomi in 2001 to work on content and media distribution over the internet. He also advised on managing their intellectual property. He retired from CU Boulder in 2010, and is now professor emeritus. You can see his work history in more detail at his web page, which Iâ€™ve linked to here.

Steve Jobs â€“ I wrote about Jobsâ€™s work in a separate [post](https://tekkie.wordpress.com/2011/10/12/remembering-steve-jobs-and-apple-computer/). He died on October 5, 2011.

Bob Belleville â€“ I couldnâ€™t find much on him. What I have is that he joined Apple in 1981, becoming the chief engineer on the Lisa, and later the Macintosh project. He later joined Silicon Graphicsâ€™s R&D Dept. (Source: [Good-bye Woz and Jobs](http://lowendmac.com/orchard/06/1002.html),Â [Making the Macintosh](http://www-sul.stanford.edu/mac/primary/docs/pr5.html))

You can watch a retrospective from 2001 given by Chuck Thacker and Butler Lampson on their days at PARC, and what they did,Â [here](http://www.youtube.com/watch?v=2H2BPrgxedY), if youâ€™re interested. It gets pretty technical, and is an hour and 20 minutes long.

You can see a 2010 interview with Adele Goldberg at the Computer History Museum, where she reviews her academic, educational, and business career [here](http://youtu.be/IGNiH85PLVg). Itâ€™s about an hour and 30 minutes.

Hereâ€™s a presentation given by the University of Texas at Austin in 2010, with Mitchell Waldrop, the author ofÂ *â€œThe Dream Machine,â€*Â andÂ Michael Hiltzik, the author ofÂ *â€œDealers of Lightning,â€*Â along with an interview with Bob Taylor. Itâ€™s a nice bookend to the history Iâ€™ve discussed here.

https://www.youtube.com/embed/jvbGAPJSDJI?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en&autohide=2&wmode=transparent

### Epilogue

The closing chapter in Waldropâ€™s book is called, â€œLickâ€™s Kids.â€ It talks about the people who were mentored by Licklider, either through ARPA, or at MIT, and who went out into the world to bring their ideas to life. It also talked about his waning years. He lived to see â€œthe wheel reinventedâ€ with microcomputers. The companies that were going gangbusters with them were repeating many of the lessons he and his researchers had learned in the 1960s, working at ARPA. The implication being that if they had merely taken time to look at what had already been learned 20 years earlier, they wouldâ€™ve avoided the same mistakes. He also saw the first glimmers of his vision of the Multinet come into being with the internet.

When reflecting on his lifeâ€™s work, Lick was humble. He didnâ€™t give himself much credit for creating our digital world. He thought of himself as just happening to be at the right place at the right time, while some very bright people did the real work. But those who were mentored, and funded by him gave him a great deal of credit. They said if it wasnâ€™t for him, their ideas would not have gotten off the ground, and often he was the *only* one who could see promise in them. He was indeed in the right place at the right time, but what was important was that he was in the right position to give them the support they needed to bring their dreams into reality.

Iâ€™ll talk more about Bob Kahn, and the development of the internet, in [Part 4](https://tekkie.wordpress.com/2017/04/17/a-history-lesson-on-government-rd-part-4/).

â€”Mark Miller, [https://tekkie.wordpress.com](https://tekkie.wordpress.com/)

This is one of a series of â€œbread crumbâ€ articles Iâ€™ve written. To see more like this, go to the [Bread Crumbs](https://tekkie.wordpress.com/bread-crumbs/) page.

